{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc66f750-bf12-4f91-a1be-37ead186d31a",
   "metadata": {},
   "source": [
    "## Hypothetical Document Embeddings (HyDE)\n",
    "\n",
    "modified from - https://github.com/langchain-ai/langchain/tree/master/cookbook\n",
    "\n",
    "HyDE creates a \"Hypothetical\" answer with the LLM and then embeds that for search\n",
    "\n",
    "HyDE = Base Embedding model+ LLM Chain (with prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbde894d-b078-42b2-824b-f980789b9a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, HypotheticalDocumentEmbedder\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98412fff-884e-4658-b77c-3273756074a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff713ff2-0615-4741-b4b4-6cd97756e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gemma2:2b\"\n",
    "ollama_embds = OllamaEmbeddings(model=model_name)\n",
    "llm = OllamaLLM(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45200de5-aa09-424d-883c-996d830b8ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with `web_search` prompt\n",
    "embeddings = HypotheticalDocumentEmbedder.from_llm(\n",
    "    llm,\n",
    "    ollama_embds,  # ollama embeddings\n",
    "    prompt_key=\"web_search\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f8a90f7-60d4-4941-9759-d6c2a5076847",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43membeddings\u001b[49m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mprompt\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings.llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d09a3c67-b614-4d4b-942c-ad4ebe645d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59e4fe36-101b-4870-b1e3-245dfa808289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:OllamaLLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Please write a passage to answer the question \\nQuestion: What items does McDonalds make?\\nPassage:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:OllamaLLM] [24.47s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"McDonald's is famous for its classic fast-food menu, featuring iconic items like french fries and hamburgers.  They are known for their Big Mac, Chicken McNuggets, Quarter Pounder, McChicken, Happy Meal toys, and a variety of other burgers, sandwiches, and breakfast items.  Beyond the core offerings, McDonald's also has salads, wraps, oatmeal, fruit cups, and beverages like milkshakes and soda.  Their menu is designed to be customizable with various toppings, sauces, and combinations.  Whether you're craving something familiar or looking for a quick bite, there's likely something at McDonald's to satisfy your hunger! \\n\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"gemma2:2b\",\n",
      "          \"created_at\": \"2024-08-21T05:42:32.3471666Z\",\n",
      "          \"response\": \"\",\n",
      "          \"done\": true,\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"context\": [\n",
      "            106,\n",
      "            1645,\n",
      "            108,\n",
      "            5958,\n",
      "            5598,\n",
      "            476,\n",
      "            14732,\n",
      "            577,\n",
      "            3448,\n",
      "            573,\n",
      "            2872,\n",
      "            235248,\n",
      "            108,\n",
      "            9413,\n",
      "            235292,\n",
      "            2439,\n",
      "            5100,\n",
      "            1721,\n",
      "            173409,\n",
      "            1501,\n",
      "            235336,\n",
      "            108,\n",
      "            147353,\n",
      "            235292,\n",
      "            107,\n",
      "            108,\n",
      "            106,\n",
      "            2516,\n",
      "            108,\n",
      "            139304,\n",
      "            235303,\n",
      "            235256,\n",
      "            603,\n",
      "            10964,\n",
      "            604,\n",
      "            1277,\n",
      "            8889,\n",
      "            4847,\n",
      "            235290,\n",
      "            11804,\n",
      "            6901,\n",
      "            235269,\n",
      "            24519,\n",
      "            34829,\n",
      "            5100,\n",
      "            1154,\n",
      "            33890,\n",
      "            50148,\n",
      "            578,\n",
      "            210499,\n",
      "            235265,\n",
      "            139,\n",
      "            4508,\n",
      "            708,\n",
      "            3836,\n",
      "            604,\n",
      "            1024,\n",
      "            7517,\n",
      "            5439,\n",
      "            235269,\n",
      "            30501,\n",
      "            63972,\n",
      "            75697,\n",
      "            235269,\n",
      "            40946,\n",
      "            79586,\n",
      "            475,\n",
      "            235269,\n",
      "            3790,\n",
      "            51788,\n",
      "            235269,\n",
      "            19196,\n",
      "            66988,\n",
      "            20369,\n",
      "            235269,\n",
      "            578,\n",
      "            476,\n",
      "            8080,\n",
      "            576,\n",
      "            1156,\n",
      "            60417,\n",
      "            235269,\n",
      "            59105,\n",
      "            235269,\n",
      "            578,\n",
      "            14457,\n",
      "            5100,\n",
      "            235265,\n",
      "            139,\n",
      "            49614,\n",
      "            573,\n",
      "            8131,\n",
      "            40841,\n",
      "            235269,\n",
      "            37757,\n",
      "            235303,\n",
      "            235256,\n",
      "            1170,\n",
      "            919,\n",
      "            80815,\n",
      "            235269,\n",
      "            60044,\n",
      "            235269,\n",
      "            94901,\n",
      "            235269,\n",
      "            9471,\n",
      "            20393,\n",
      "            235269,\n",
      "            578,\n",
      "            55848,\n",
      "            1154,\n",
      "            2658,\n",
      "            63158,\n",
      "            2207,\n",
      "            578,\n",
      "            30886,\n",
      "            235265,\n",
      "            139,\n",
      "            27771,\n",
      "            6901,\n",
      "            603,\n",
      "            6869,\n",
      "            577,\n",
      "            614,\n",
      "            93257,\n",
      "            675,\n",
      "            4282,\n",
      "            117877,\n",
      "            235269,\n",
      "            94519,\n",
      "            235269,\n",
      "            578,\n",
      "            29862,\n",
      "            235265,\n",
      "            139,\n",
      "            36049,\n",
      "            692,\n",
      "            235303,\n",
      "            478,\n",
      "            93303,\n",
      "            2775,\n",
      "            10709,\n",
      "            689,\n",
      "            3648,\n",
      "            604,\n",
      "            476,\n",
      "            4320,\n",
      "            26911,\n",
      "            235269,\n",
      "            1104,\n",
      "            235303,\n",
      "            235256,\n",
      "            5476,\n",
      "            2775,\n",
      "            696,\n",
      "            37757,\n",
      "            235303,\n",
      "            235256,\n",
      "            577,\n",
      "            24439,\n",
      "            861,\n",
      "            38326,\n",
      "            235341,\n",
      "            235248,\n",
      "            108\n",
      "          ],\n",
      "          \"total_duration\": 24394006600,\n",
      "          \"load_duration\": 48815200,\n",
      "          \"prompt_eval_count\": 30,\n",
      "          \"prompt_eval_duration\": 187250000,\n",
      "          \"eval_count\": 137,\n",
      "          \"eval_duration\": 24153374000\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Now we can use it as any embedding class!\n",
    "result = embeddings.embed_query(\"What items does McDonalds make?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8940998b-41df-4bef-81cc-de1bc5b43bf6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Using own prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dde17eb-5d62-4977-91d0-3d22934425e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\rag-f3KTP8yE-py3.12\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"Please answer the user's question as a single food item\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"question\"], template=prompt_template)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f65d1c5-55a6-450a-a878-06f41875312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HypotheticalDocumentEmbedder(\n",
    "    llm_chain=llm_chain,\n",
    "    # base_embeddings=bge_embeddings\n",
    "    base_embeddings=ollama_embds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e129f04f-81d6-4ea8-828c-e330a95b4e8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:OllamaLLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Please answer the user's question as a single food item\\nQuestion: What is is McDonalds best selling item?\\nAnswer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:OllamaLLM] [3.94s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"**Big Mac** üçî \\n\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"gemma2:2b\",\n",
      "          \"created_at\": \"2024-08-21T05:43:34.4961947Z\",\n",
      "          \"response\": \"\",\n",
      "          \"done\": true,\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"context\": [\n",
      "            106,\n",
      "            1645,\n",
      "            108,\n",
      "            5958,\n",
      "            3448,\n",
      "            573,\n",
      "            2425,\n",
      "            235303,\n",
      "            235256,\n",
      "            2872,\n",
      "            685,\n",
      "            476,\n",
      "            3821,\n",
      "            2960,\n",
      "            2599,\n",
      "            108,\n",
      "            9413,\n",
      "            235292,\n",
      "            2439,\n",
      "            603,\n",
      "            603,\n",
      "            173409,\n",
      "            1963,\n",
      "            13026,\n",
      "            2599,\n",
      "            235336,\n",
      "            108,\n",
      "            1261,\n",
      "            235292,\n",
      "            107,\n",
      "            108,\n",
      "            106,\n",
      "            2516,\n",
      "            108,\n",
      "            688,\n",
      "            8267,\n",
      "            5439,\n",
      "            688,\n",
      "            235248,\n",
      "            244174,\n",
      "            235248,\n",
      "            108\n",
      "          ],\n",
      "          \"total_duration\": 3929511100,\n",
      "          \"load_duration\": 39329800,\n",
      "          \"prompt_eval_count\": 35,\n",
      "          \"prompt_eval_duration\": 2428892000,\n",
      "          \"eval_count\": 9,\n",
      "          \"eval_duration\": 1457901000\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = embeddings.embed_query(\n",
    "    \"What is is McDonalds best selling item?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512a0d6b-ff25-4fe9-a2d9-044b936af38c",
   "metadata": {
    "id": "nsVi5ucMilq1"
   },
   "source": [
    "## Using HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63160c71-99e8-400f-ae42-53a4db6bb5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1244, which is longer than the specified 1000\n",
      "Created a chunk of size 1050, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# with open(\"../../state_of_the_union.txt\") as f:\n",
    "#     state_of_the_union = f.read()\n",
    "\n",
    "loaders = [\n",
    "    TextLoader('./data//whats_next_for_podium.txt',\n",
    "               encoding='UTF8'),\n",
    "]\n",
    "docs = []\n",
    "for l in loaders:\n",
    "    docs.extend(l.load())\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "texts = text_splitter.split_documents(docs) #split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9dddea5-82b0-45a4-a11e-1ee07abdbef7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data//whats_next_for_podium.txt'}, page_content=\"How Podium optimized agent behavior and reduced engineering intervention by 90% with LangSmith\\nSee how Podium tests across the lifecycle development of their AI employee agent, using LangSmith for dataset curation and finetuning. They improved agent F1 response quality to 98% and reduced the need for engineering intervention by 90%.\\n\\n5 min read\\nAug 15, 2024\\nAbout Podium\\nPodium is a communication platform that helps small businesses connect quickly with customers via phone, text, email, and social media. Small businesses often have high-touch interactions with customers ‚Äî think automotive dealers, jewelers, bike shops ‚Äî yet are understaffed. Podium's mission is to help these businesses respond to customer inquiries promptly so that they can convert leads into sales.\"),\n",
       " Document(metadata={'source': './data//whats_next_for_podium.txt'}, page_content='Podium data shows that responding to customer inquiries within 5 minutes results in a 46% higher lead conversion rate than responding in an hour. To improve lead capture, Podium launched AI Employee, their agentic application (and flagship product) to engage local business customers, schedule appointments, and close sales. \\n\\nInitially, Podium used the LangChain framework for single-turn interactions. As their agentic use cases grew more complex for a wide-ranging set of customers and domains, Podium needed better visibility into their LLM calls and interactions ‚Äî and turned to LangSmith for LLM testing and observability.\\n\\nTesting across the agentic development lifecycle\\nEstablishing feedback loops was especially important to the agentic development lifecycle for Podium. LangSmith allowed the Podium engineers to test and continuously monitor their AI employee‚Äôs performance, adding new edge cases to their dataset to refine and test the model over time.'),\n",
       " Document(metadata={'source': './data//whats_next_for_podium.txt'}, page_content='Podium‚Äôs testing approach looks like the following:'),\n",
       " Document(metadata={'source': './data//whats_next_for_podium.txt'}, page_content=\"Baseline Dataset Curation: Create an initial dataset to represent basic use cases and requirements for the agent. This serves as a foundation for testing and development.\\nBaseline Offline Evaluation: Conduct initial tests using the curated dataset to assess the agent's performance against the basic requirements before shipping to production.\\nCollecting Feedback: \\nUser-Provided Feedback: Collect direct input from users interacting with the agent. \\nOnline Evaluation: Use LLMs to self-evaluate and monitor the quality of responses using in real-time, flagging potential issues for further investigation.\\nOptimization: \\nPrompt Tuning: Refine the prompts used to guide the agent's responses.\\nRetrieval Tuning: Adjust the retrieval mechanisms used to generate responses.\\nModel Fine-Tuning: Use traced data to further train and specialize the model for specific tasks.\\nOngoing Evaluation: \\nOffline Evaluation: Evaluate the agent's performance and identify opportunities for optimization using backtesting, pairwise comparisons, and other testing methods.\\nDataset Curation: Continuously update and expand the test dataset with new scenarios and edge cases for regression testing, ensuring new changes don't negatively impact existing capabilities.\"),\n",
       " Document(metadata={'source': './data//whats_next_for_podium.txt'}, page_content='How Podium creates testing loops for their agent \\nDataset curation and fine-tuning agents with LangSmith\\nPrior to LangSmith, understanding a customer inquiry and what steps employees should take to resolve the inquiry was difficult, since the Podium engineers made 20-30 LLM calls per interaction. With LangSmith, they quickly got set up and logged and viewed traces to aggregate insights.\\n\\nOne specific challenge Podium ran into with their AI Employee was that the agent struggled to recognize when a conversation had naturally ended, resulting in awkward repeated goodbyes. To address this, Podium began by creating a dataset in LangSmith with various conversation scenarios, including ways different conversations might conclude.'),\n",
       " Document(metadata={'source': './data//whats_next_for_podium.txt'}, page_content='Their engineering team then found it helpful to upgrade to a larger model, curating the outputs into a smaller model (using a technique called model distillation). Upgrading their model went smoothly since model inputs and outputs were automatically captured in LangSmith‚Äôs traces, allowing the team to easily curate datasets.\\n\\nPodium engineers also enriched LangSmith traces with metadata on customer profiles, business types, and other parameters important to their business. They grouped traces using specific identifiers in LangSmith, making it easy to aggregate related traces during data curation. This enriched data enabled Podium to create a higher-quality and balanced dataset, which improved model fine-tuning and helped them avoid overfitting).'),\n",
       " Document(metadata={'source': './data//whats_next_for_podium.txt'}, page_content='With this balanced dataset, the Podium team then compared the results from their fine-tuned model against results from their original, larger model using pairwise evaluations. This comparison allowed them to assess how well the upgraded model could improve the agent‚Äôs ability to know when to conclude a conversation.\\n\\nAfter fine-tuning, Podium‚Äôs new model showed significant improvement in detecting where natural conversation should end for its agent. Podium‚Äôs F1 scores with the fine-tune model experienced a 7.5% improvement, going from 91.7% to 98.6% to exceed their quality threshold of 98%.\\n\\nHigh-quality customer support for AI platform without engineering intervention\\nAt Podium, engineers must understand when communications with customers go awry, so that they can keep shipping reliable and high-quality products.'),\n",
       " Document(metadata={'source': './data//whats_next_for_podium.txt'}, page_content='Since publicly launching their AI Employee in January, it became critical for the Technical Product Specialists (TPS) at Podium to troubleshoot issues users were encountering in real-time. At Podium, the TPS team typically provides customer support for their small business customers. However, pinpointing the source of issues (and how to take action on them) was challenging. \\n\\nGiving the TPS team access to LangSmith provided clarity, allowing the team to quickly identify customer-reported issues and determine: ‚ÄúIs this issue caused by a bug in the application, incomplete context, misaligned instructions, or an issue with the LLM?‚Äù \\n\\nFor Podium, identifying the type of customer issue guided them to the appropriate interventions:'),\n",
       " Document(metadata={'source': './data//whats_next_for_podium.txt'}, page_content='For bugs in the application: These are orchestration failures, such as an integration failing to return data. These require engineering intervention.\\nFor incomplete context: LLM is missing information needed to answer a question. These can be remediated by the TPS team by adding additional content.\\nFor misaligned instructions: Instructions are based on business requirements; any issues in the requirements can affect agent behavior. These can be remediated by the TPS team making changes in the content authoring system to better suit business requirements.\\nFor an LLM issue: Even with necessary context, an LLM may produce unexpected or incorrect information. These require engineering intervention.\\nFor example, many car dealerships use Podium‚Äôs AI Employee to respond to customer inquiries. If the AI Employee mistakenly responds that a car dealership does not offer oil changes, the TPS team can use LangSmith‚Äôs playground feature to edit the system output and determine if a simple setting change in the Admin interface can resolve the issue.'),\n",
       " Document(metadata={'source': './data//whats_next_for_podium.txt'}, page_content='LangSmith Playground enables Podium‚Äôs support team to troubleshoot agent behavior without engineering intervention\\nBefore LangSmith, troubleshooting agent behavior often required engineering intervention. This was a time-consuming process that involved calling in engineers to first review model inputs and outputs, and then rewrite and refactor the code.\\n\\nBy giving their TPS team access to LangSmith traces, Podium has reduced the need for engineering intervention by 90%, allowing their engineers to focus more on development instead of support tasks.\\n\\nIn summary, using LangSmith led to:'),\n",
       " Document(metadata={'source': './data//whats_next_for_podium.txt'}, page_content='Increased efficiency of Podium‚Äôs support team by enabling them to resolve issues more quickly and independently.\\nImproved customer satisfaction (CSAT) scores for both support interactions and Podium‚Äôs AI-powered services.\\nWhat‚Äôs Next for Podium\\nBy integrating LangSmith and LangChain, Podium has gained a competitive edge in the space of customer experience tools. LangSmith has enhanced observability and simplified the management of large datasets and optimizing model performance. The Podium team has also been integrating LangGraph into its workflow, reducing complexity in their agent orchestration while serving different target customers, while increasing controllability over their agent conversations. \\n\\nTogether, these suite of products have allowed Podium to focus on their core value proposition ‚Äî help small businesses capture leads more effectively ‚Äî and efficiently design, test, and monitor their LLM applications.'),\n",
       " Document(metadata={'source': './data//whats_next_for_podium.txt'}, page_content='Podium is hiring across roles to help local businesses win. Inspired by Podium‚Äôs story? You can also try out LangSmith for free or talk to a LangSmith expert to learn more. \\n\\nAnd for a more comprehensive best practices for testing and evaluating your LLM application, check out this guidebook.\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b42c6839-d0ef-44b7-a44a-ca23c6fd5395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\huggingface\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"Please answer the user's question as related to Large Language Models\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"question\"], template=prompt_template)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2271feed-8b75-4857-a15f-7a2c81726b22",
   "metadata": {
    "id": "A-xmDGJEdb_K"
   },
   "outputs": [],
   "source": [
    "embeddings = HypotheticalDocumentEmbedder(\n",
    "    llm_chain=llm_chain,\n",
    "    base_embeddings=ollama_embds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f855e033-1436-4fe4-a527-433a8f0761ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVwwG28ki5-A",
    "outputId": "bf1f060c-6eeb-4c15-d0f7-377c22db0ccc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea38611-87bf-446d-b7ca-bc8798e3f29e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVwwG28ki5-A",
    "outputId": "bf1f060c-6eeb-4c15-d0f7-377c22db0ccc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"What is podium?\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4069eb9-c7cb-4d36-89d6-6e4b7dd6262d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pNCA7hxjK4B",
    "outputId": "81206283-7c4d-42e3-c866-4ba78e464c74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Their engineering team then found it helpful to upgrade to a larger model, curating the outputs into a smaller model (using a technique called model distillation). Upgrading their model went smoothly since model inputs and outputs were automatically captured in LangSmith‚Äôs traces, allowing the team to easily curate datasets.\n",
      "\n",
      "Podium engineers also enriched LangSmith traces with metadata on customer profiles, business types, and other parameters important to their business. They grouped traces using specific identifiers in LangSmith, making it easy to aggregate related traces during data curation. This enriched data enabled Podium to create a higher-quality and balanced dataset, which improved model fine-tuning and helped them avoid overfitting).\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5f97c01-bb51-4c32-8a44-b69790c1ac38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided text, **Podium** appears to be a company or organization that develops and uses AI models. \\n\\nHere\\'s why:\\n\\n* **\"Their engineering team...\"**:  This suggests Podium has an engineering team responsible for developing and deploying their AI model.\\n* **\"Upgrading their model...\"**: This implies they are actively involved in building, modifying, and improving their AI model. \\n* **\"LangSmith traces...\"**: LangSmith is likely a tool or system that captures data and helps with the process of building and fine-tuning the AI model.\\n* **\"Enriched LangSmith traces...\"**:  This indicates they are using LangSmith to add additional information to the data, making it more usable for their AI models.\\n\\n\\nTherefore, based on the context provided, Podium is likely an organization involved in developing and implementing AI models. \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":docs[0].page_content,\"question\":query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27d40b10-7f2b-4217-a2d8-ee418b60eb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context provided, **Podium** seems to be an engineering team or organization that develops and uses language models. \\n\\nHere\\'s why:\\n\\n* **They use LangSmith:** This suggests Podium has a specific system for processing language model data (LangSmith might be a tool or platform).\\n* **Curating datasets:**  The context mentions the engineering team \"curating outputs\" and \"grouping traces\" suggesting they work with large amounts of data. \\n* **Model fine-tuning:** The team enhances their models with curated datasets, highlighting a focus on training language models for specific tasks (like customer service or business information analysis).\\n\\nTherefore, Podium likely focuses on building and applying sophisticated language models for various purposes like customer service interactions and business intelligence analysis.  \\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rag_chain.invoke({\"context\":docs[0].page_content,\"question\":query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fbd2d6-73c7-4656-8044-4881b0f322ae",
   "metadata": {},
   "source": [
    "## Comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083b0496-a3f6-4bf4-b50d-8ca22331aaf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42896532-fc66-42b8-aa84-5241ca7f03b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    texts,\n",
    "    OllamaEmbeddings(model=model_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c5554-6977-475f-923e-80d55dc42fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a93b0db7-63ac-4a71-91fe-e2396618acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9c6dc38-bfca-438f-bdd5-ba095ba1783c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c44d24c-abf6-4a3f-90a4-e532fe04abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    # Í≤ÄÏÉâÌïú Î¨∏ÏÑú Í≤∞Í≥ºÎ•º ÌïòÎÇòÏùò Î¨∏Îã®ÏúºÎ°ú Ìï©Ï≥êÏ§çÎãàÎã§.\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Ï≤¥Ïù∏ÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a24deed9-049d-4b56-9311-0028c4a85d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Podium is a communication platform designed to help small businesses efficiently connect with their customers through phone, text, email, and social media. Their goal is to enable these businesses to handle high-touch interactions like those found in automotive dealerships, jewelry stores, or bike shops, ultimately helping them convert leads into sales. \\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb2cdd-242e-4bb8-8be6-995077469401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ef466-f0ee-4dea-9672-382a8302b0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd8803e-9717-4844-8fc3-20cf89333a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7237e2-75e8-48d1-a568-e0e02dcfbb07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
